{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEzmO3mNwvQH"
   },
   "source": [
    "*#MARS2020*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cec1CyBDO2X"
   },
   "source": [
    "# **Model Creation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hE-J5P3WNsZw"
   },
   "source": [
    "##### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "-TG9rFMOrgpx"
   },
   "outputs": [],
   "source": [
    "#MARS2020\n",
    "#Create a clothing image classification network using fashion_mnist data set\n",
    "#data set contains 70,000 images divided into 10 categories\n",
    "#60,000 of these images will be fed into the training model while 10,000 will be used to test model accuracy\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow.keras as tfk\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"savefig.format\"] = 'png'\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "fashion_mnist = tfk.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "#Normalize the value of images\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "(x_train, x_valid) = train_images[10000:], train_images[:10000] \n",
    "(y_train, y_valid) = train_labels[10000:], train_labels[:10000]\n",
    "\n",
    "#class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTOTVBreNvb3"
   },
   "source": [
    "##### Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "5SIfX3iGrgqC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MAIN MODEL FUNCTION\n",
    "\n",
    "def fashion(n_run, n_reps, n_layers, n_neurons, n_batch, n_epochs):\n",
    "    print('\\n\\nTEST FOR VARIATION ' + str(n_run))\n",
    "\n",
    "    global train_acc_ave\n",
    "    global train_loss_ave\n",
    "    global valid_acc_ave\n",
    "    global valid_loss_ave\n",
    "    global test_acc_ave\n",
    "    global test_loss_ave\n",
    "    global result\n",
    "    \n",
    "    train_acc_list = []\n",
    "    train_loss_list = []\n",
    "    valid_acc_list = []\n",
    "    valid_loss_list = []\n",
    "    test_acc_list = []\n",
    "    test_loss_list = []\n",
    "    \n",
    "    for rep in range(n_reps):\n",
    "        print(\"\\nTEST RUN \" + str(n_run) + \", REP\" + \" \" + str(rep+1))\n",
    "        \n",
    "        #Building the Neural Network Architecture\n",
    "        model = tfk.Sequential()\n",
    "        model.add(tfk.layers.Flatten(input_shape=(28, 28)))\n",
    "        for z in range(n_layers):\n",
    "            model.add(tfk.layers.Dense(n_neurons, activation='relu'))\n",
    "        model.add(tfk.layers.Dense(10))\n",
    "\n",
    "        model.compile(optimizer='adam', loss=tfk.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "        \n",
    "        #Training the Model\n",
    "        history = model.fit(x_train, y_train, batch_size=n_batch, epochs=n_epochs, validation_data=(x_valid,  y_valid))\n",
    "        train_acc = history.history['accuracy']\n",
    "        train_loss = history.history['loss']\n",
    "        train_acc_list.append(round(train_acc[-1]*100, 2))\n",
    "        train_loss_list.append(round(train_loss[-1]*100, 2))\n",
    "        \n",
    "        valid_acc = history.history['val_accuracy']\n",
    "        valid_loss = history.history['val_loss']\n",
    "        valid_acc_list.append(round(valid_acc[-1]*100, 2))\n",
    "        valid_loss_list.append(round(valid_loss[-1]*100, 2))\n",
    "        \n",
    "        x_axis = list(range(1, n_epochs+1))\n",
    "        \n",
    "        #Plotting model accuracy and loss        \n",
    "        plt.figure(figsize=(20,5)).patch.set_facecolor('white')\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(x_axis, train_acc, marker='o')\n",
    "        plt.plot(x_axis, valid_acc, marker='o')\n",
    "        plt.title('model accuracy')\n",
    "        plt.xticks(x_axis)\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['training', 'validation'], loc='upper left')\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(x_axis, train_loss, marker='o')\n",
    "        plt.plot(x_axis, valid_loss, marker='o')\n",
    "        plt.title('model loss')\n",
    "        plt.xticks(x_axis)\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['training', 'validation'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "        #Evaluate Training Accuracy\n",
    "        print(\"Test 10000 images: \")\n",
    "        test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "        test_acc_list.append(round(test_acc*100, 2))\n",
    "        test_loss_list.append(round(test_loss*100, 2))\n",
    "\n",
    "        #Making Predictions\n",
    "        probability_model = tfk.Sequential([model, tfk.layers.Softmax()])\n",
    "        predictions = probability_model.predict(test_images) \n",
    "    \n",
    "    train_acc_ave = round(mean(train_acc_list), 2)\n",
    "    train_loss_ave = round(mean(train_loss_list), 2)\n",
    "    valid_acc_ave = round(mean(valid_acc_list), 2)\n",
    "    valid_loss_ave = round(mean(valid_loss_list), 2)\n",
    "    test_acc_ave = round(mean(test_acc_list), 2)\n",
    "    test_loss_ave = round(mean(test_loss_list), 2)\n",
    "    \n",
    "    print('\\nVARIATION '  + str(n_run) + ' MODEL SUMMARY')\n",
    "    model.summary()\n",
    "    \n",
    "    print('\\nTEST RUN ' + str(n_run) + ' RESULTS')\n",
    "    line1 = \"Training Accuracy: \" + str(train_acc_list) + '  /  Average = ' + str(train_acc_ave)\n",
    "    line2 = \"\\nTraining Loss: \" + str(train_loss_list) + '  /  Average = ' + str(train_loss_ave)\n",
    "    line3 = \"\\nValidation Accuracy: \" + str(valid_acc_list) + '  /  Average = ' + str(valid_acc_ave)\n",
    "    line4 = \"\\nValidation Loss: \" + str(valid_loss_list) + '  /  Average = ' + str(valid_loss_ave)\n",
    "    line5 = \"\\nTesting Accuracy: \" + str(test_acc_list) + '  /  Average = ' + str(test_acc_ave)\n",
    "    line6 = \"\\nTesting Loss: \" + str(test_loss_list) + '  /  Average = ' + str(test_loss_ave)\n",
    "    result = line1 + line2 + line3 + line4 + line5 + line6\n",
    "    print(result + '\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24YTvQFhDa_F"
   },
   "source": [
    "# **Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Call Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JYaApYC8rgqD",
    "outputId": "44a7a41d-04da-4c6c-a358-446ae27c629c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "repetitions = int(input(\"Enter number of test repetitions: \"))\n",
    "variable = int(input(\"Choose variable:\\n1: hidden layer\\n2: neuron\\n3: batch size\\n4: epoch\\n\"))\n",
    "\n",
    "train_acc_array = []\n",
    "train_loss_array = []\n",
    "valid_acc_array = []\n",
    "valid_loss_array = []\n",
    "test_acc_array = []\n",
    "test_loss_array = []\n",
    "result_array = []\n",
    "        \n",
    "def t_append():\n",
    "    train_acc_array.append(train_acc_ave)\n",
    "    train_loss_array.append(train_loss_ave)\n",
    "    valid_acc_array.append(valid_acc_ave)\n",
    "    valid_loss_array.append(valid_loss_ave)\n",
    "    test_acc_array.append(test_acc_ave)\n",
    "    test_loss_array.append(test_loss_ave)\n",
    "    result_array.append(result)\n",
    "\n",
    "if variable == 1:\n",
    "    res_list = [int(x) for x in list(input(\"Enter list of hidden layers: \").split(' '))]\n",
    "    input1 = int(input(\"Enter number of neurons: \"))\n",
    "    con1 = \"NEURON(S)\"\n",
    "    input2 = int(input(\"Enter batch size: \"))\n",
    "    con2 = \"SAMPLE(S) PER BATCH\"\n",
    "    input3 = int(input(\"Enter number of epochs: \"))\n",
    "    con3 = \"EPOCH(S)\"\n",
    "    var = \"HIDDEN LAYER(S)\"\n",
    "    for i in range(len(res_list)):\n",
    "        fashion(i+1, repetitions, res_list[i], input1, input2, input3)\n",
    "        t_append()\n",
    "elif variable == 2:\n",
    "    res_list = [int(x) for x in list(input(\"Enter list of neurons: \").split(' '))]\n",
    "    input1 = int(input(\"Enter number of hidden layers: \"))\n",
    "    con1 = \"HIDDEN LAYER(S)\"\n",
    "    input2 = int(input(\"Enter batch size: \"))\n",
    "    con2 = \"SAMPLE(S) PER BATCH\"\n",
    "    input3 = int(input(\"Enter number of epochs: \"))\n",
    "    con3 = \"EPOCH(S)\"\n",
    "    var = 'NEURON(S)'\n",
    "    for i in range(len(res_list)):\n",
    "        fashion(i+1, repetitions, input1, res_list[i], input2, input3)\n",
    "        t_append()\n",
    "elif variable == 3:\n",
    "    res_list = [int(x) for x in list(input(\"Enter list of batch sizes: \").split(' '))]\n",
    "    input1 = int(input(\"Enter number of hidden layers: \"))\n",
    "    con1 = \"HIDDEN LAYER(S)\"\n",
    "    input2 = int(input(\"Enter number of neurons: \"))\n",
    "    con2 = \"NEURON(S)\"\n",
    "    input3 = int(input(\"Enter number of epochs: \"))\n",
    "    con3 = \"EPOCH(S)\"\n",
    "    var = 'SAMPLE(S) PER BATCH'\n",
    "    for i in range(len(res_list)):\n",
    "        fashion(i+1, repetitions, input1, input2, res_list[i], input3)\n",
    "        t_append()\n",
    "elif variable == 4:\n",
    "    res_list = [int(x) for x in list(input(\"Enter list of epochs: \").split(' '))]\n",
    "    input1 = int(input(\"Enter number of hidden layers: \"))\n",
    "    con1 = \"HIDDEN LAYER(S)\"\n",
    "    input2 = int(input(\"Enter number of neurons: \"))\n",
    "    con2 = \"NEURON(S)\"\n",
    "    input3 = int(input(\"Enter batch size: \"))\n",
    "    con3 = \"SAMPLE(S) PER BATCH\"\n",
    "    var = 'EPOCH(S)'\n",
    "    for i in range(len(res_list)):\n",
    "        fashion(i+1, repetitions, input1, input2, input3, res_list[i])\n",
    "        t_append()\n",
    "else:\n",
    "    print('NOT VALID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWlN5EnADitg"
   },
   "source": [
    "# **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdrUEiTtOEek"
   },
   "source": [
    "##### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5yl-ntU0rgqF",
    "outputId": "701e4811-f1b4-4ccf-feb7-8e8c800573df"
   },
   "outputs": [],
   "source": [
    "print('RESULTS:')\n",
    "print('\\n' + str(repetitions) + ' ' + \"REPETITION(S)\\n\")\n",
    "print(input1, con1)\n",
    "print(input2, con2)\n",
    "print(input3, con3)\n",
    "print('\\n\\n' + str(len(res_list)) + ' VARIATION(S):')\n",
    "for n in range(len(res_list)):\n",
    "    print('\\n' + str(res_list[n]) + ' ' + var)\n",
    "    print(result_array[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCpa5oVaOIEg"
   },
   "source": [
    "##### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels(y_array, y_dist):\n",
    "    for x,y in zip(res_list, y_array):\n",
    "        label = \"{:.2f}\".format(y)\n",
    "        plt.annotate(label, (x,y), textcoords=\"offset points\", xytext=(0,y_dist), ha='center')\n",
    "        \n",
    "params = '\\n\\n' + str(input1) + ' ' + con1 + '; ' + str(input2) + ' ' + con2 + '; ' + str(input3) + ' ' + con3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def val_plot(print_x, acc_array, loss_array, title, ylabel='AVERAGE'):\n",
    "    print('\\n' + print_x)\n",
    "    print('Accuracy: ' + str(acc_array))\n",
    "    print('Loss: ' + str(loss_array))\n",
    "    plt.figure(figsize=(12,8)).patch.set_facecolor('white')\n",
    "    plt.plot(res_list, acc_array, color='green', marker='o', markersize=10)\n",
    "    plt.plot(res_list, loss_array, color='blue', marker='o', markersize=10)\n",
    "    plt.title(title)\n",
    "    plt.xticks(res_list)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(var+params)\n",
    "    plt.ylim((0,100))\n",
    "    labels(acc_array, 10)\n",
    "    labels(loss_array, 10)\n",
    "    plt.legend(['accuracy', 'loss'], loc='lower left')\n",
    "    filename = var + \" \" + title\n",
    "    #plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "print('SUMMARY:\\n')\n",
    "print(str(repetitions) + ' ' + \"REPETITION(S)\\n\")\n",
    "print(input1, con1)\n",
    "print(input2, con2)\n",
    "print(input3, con3)\n",
    "print('\\nVARIABLE: ' + var)\n",
    "print(str(len(res_list)) + ' VARIATIONS')\n",
    "print(str(res_list))\n",
    "val_plot('Training Average:', train_acc_array, train_loss_array, 'TRAINING')\n",
    "val_plot('Validation Average:', valid_acc_array, valid_loss_array, 'VALIDATION')\n",
    "val_plot('Testing Average:', test_acc_array, test_loss_array, 'TESTING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def combi(train, valid, test, title,loc):\n",
    "    plt.figure(figsize=(15,7)).patch.set_facecolor('white')\n",
    "    plt.plot(res_list, train, marker='o', markersize=7)\n",
    "    labels(train, 5)\n",
    "    plt.plot(res_list, valid, marker='o', markersize=7)\n",
    "    labels(valid, 5)\n",
    "    plt.plot(res_list, test, marker='o', markersize=7)\n",
    "    labels(test, 5)\n",
    "    plt.title(title)\n",
    "    plt.xticks(res_list)\n",
    "    plt.ylabel('AVERAGE')\n",
    "    plt.xlabel(var+params)\n",
    "    plt.legend(['training', 'validation', 'testing'], loc=loc)\n",
    "    #plt.savefig(var+' '+title, dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "combi(train_acc_array, valid_acc_array, test_acc_array, 'ACCURACY','lower right')\n",
    "combi(train_loss_array, valid_loss_array, test_loss_array, 'LOSS','upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Okj5N93OTQ8"
   },
   "source": [
    "##### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5p2AdI5OSm8",
    "outputId": "70c07905-e657-4138-f3dd-ca7928bea2b7"
   },
   "outputs": [],
   "source": [
    "data = {var:res_list,'Training Accuracy':train_acc_array,'Training Loss':train_loss_array,\n",
    "        'Validation Accuracy':valid_acc_array,'Validation Loss':valid_loss_array,\n",
    "        'Testing Accuracy':test_acc_array,'Testing Loss':test_loss_array}\n",
    "data_df = pd.DataFrame(data)\n",
    "print(data_df)\n",
    "#data_df.to_csv(var+'.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_env",
   "language": "python",
   "name": "deep_learning_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
