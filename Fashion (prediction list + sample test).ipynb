{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEzmO3mNwvQH"
   },
   "source": [
    "*#MARS2020*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCnu3qjjAWgr"
   },
   "source": [
    "##### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-93P-HR-ARaZ"
   },
   "outputs": [],
   "source": [
    "#MARS\n",
    "#Create a clothing image classification network using fashion_mnist data set\n",
    "#data set contains 70,000 images divided into 10 categories\n",
    "#60,000 of these images will be fed into the training model while 10,000 will be used to test model accuracy\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow.keras as tfk\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"savefig.format\"] = 'png'\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "fashion_mnist = tfk.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "#Normalize the value of images\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "(x_train, x_valid) = train_images[10000:], train_images[:10000]\n",
    "(y_train, y_valid) = train_labels[10000:], train_labels[:10000]\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "data_check = pd.read_csv('Fashion\\check.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assigning Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_layers = 1\n",
    "n_neurons = 256\n",
    "batch_size = 100\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOkPv8BeAYut"
   },
   "source": [
    "##### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UboA9ZAAc8b"
   },
   "outputs": [],
   "source": [
    "params = str(n_hidden_layers)+' hidden layer(s); '+str(n_neurons)+' neuron(s); '+str(batch_size)+' sample(s)/batch; '+str(n_epochs)+' epoch(s)'\n",
    "\n",
    "#MAIN MODEL FUNCTION\n",
    "#Building the Neural Network Architecture\n",
    "model = tfk.Sequential()\n",
    "model.add(tfk.layers.Flatten(input_shape=(28, 28)))\n",
    "for z in range(n_hidden_layers):\n",
    "    model.add(tfk.layers.Dense(n_neurons, activation='relu'))\n",
    "model.add(tfk.layers.Dense(10))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss=tfk.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=['accuracy'])\n",
    "        \n",
    "#Training the Model\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epochs, validation_data=(x_valid,  y_valid))\n",
    "train_acc = history.history['accuracy']\n",
    "train_loss = history.history['loss']\n",
    "valid_acc = history.history['val_accuracy']\n",
    "valid_loss = history.history['val_loss']\n",
    "x_axis = list(range(1, n_epochs+1))\n",
    "#Plotting model accuracy and loss        \n",
    "def labels(y_array):\n",
    "    for x,y in zip(x_axis, y_array):\n",
    "        label = \"{:.2f}\".format(y)\n",
    "        plt.annotate(label, (x,y), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "plt.figure(figsize=(15,7)).patch.set_facecolor('white')\n",
    "plt.plot(x_axis, train_acc, marker='o')\n",
    "labels(train_acc)\n",
    "plt.plot(x_axis, valid_acc, marker='o')\n",
    "labels(valid_acc)\n",
    "plt.title('MODEL ACCURACY\\n\\n'+params)\n",
    "plt.xticks(x_axis)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "plt.figure(figsize=(15,7)).patch.set_facecolor('white')\n",
    "plt.plot(x_axis, train_loss, marker='o')\n",
    "labels(train_loss)\n",
    "plt.plot(x_axis, valid_loss, marker='o')\n",
    "labels(valid_loss)\n",
    "plt.title('MODEL LOSS\\n\\n'+params)\n",
    "plt.xticks(x_axis)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='upper left')\n",
    "#plt.savefig(\"Model History\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "#Evaluate Training Accuracy\n",
    "print(\"Test 10000 images: \")\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "#Making Predictions\n",
    "probability_model = tfk.Sequential([model, tfk.layers.Softmax()])\n",
    "predictions = probability_model.predict(test_images)\n",
    "    \n",
    "pred_list = []\n",
    "conf_list = []\n",
    "w_conf_list = []\n",
    "index_list = []\n",
    "true_list = []\n",
    "wrong_list = []\n",
    "    \n",
    "t_conf = 0\n",
    "w_count = 0\n",
    "\n",
    "for i in range(10000):\n",
    "    predicted_label = np.argmax(predictions[i])\n",
    "    confidence = round(100*np.max(predictions[i]), 2)\n",
    "    t_conf += confidence\n",
    "    pred_list.append(class_names[predicted_label])\n",
    "    conf_list.append(confidence)\n",
    "    if test_labels[i] != predicted_label:\n",
    "        w_count += 1\n",
    "        index_list.append(i)\n",
    "        true_list.append(class_names[test_labels[i]])\n",
    "        wrong_list.append(class_names[predicted_label])\n",
    "        w_conf_list.append(confidence)\n",
    "\n",
    "conf_mean = round(t_conf/10000, 2)\n",
    "\n",
    "data_check['Predicted Label'] = pred_list\n",
    "data_check['Confidence'] = conf_list\n",
    "wrong = {'Index':index_list,'True Label':true_list,'Predicted Label':wrong_list,'Confidence':w_conf_list}\n",
    "wrong_pred = pd.DataFrame(wrong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MODEL SUMMARY:\\n')\n",
    "print('Input Shape: ' + str(model.input_shape))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val1 = round(train_acc[-1]*100, 2)\n",
    "val2 = round(train_loss[-1]*100, 2)\n",
    "val3 = round(valid_acc[-1]*100, 2)\n",
    "val4 = round(valid_loss[-1]*100, 2)\n",
    "val5 = round(test_acc*100, 2)\n",
    "val6 = round(test_loss*100, 2)\n",
    "\n",
    "print(\"Training Accuracy: \" + str(val1))\n",
    "print(\"Training Loss: \" + str(val2))\n",
    "print(\"Validation Accuracy: \" + str(val3))\n",
    "print(\"Validation Loss: \" + str(val4))\n",
    "print(\"Testing Accuracy: \" + str(val5))\n",
    "print(\"Testing Loss: \" + str(val6))\n",
    "#print(\"Confidence Mean: \" + str(conf_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#vals = [val1, val2, val3, val4, val5, val6, conf_mean]\n",
    "vals = [val1, val2, val3, val4, val5, val6]\n",
    "#x = ['Training Accuracy','Training Loss','Validation Accuracy','Validation Loss',\n",
    "     #'Testing Accuracy','Testing Loss','Confidence Mean']\n",
    "x = ['Training Accuracy','Training Loss','Validation Accuracy','Validation Loss','Testing Accuracy','Testing Loss']\n",
    "ind = np.arange(len(vals))\n",
    "fig, ax = plt.subplots(figsize = (15,5))\n",
    "fig.patch.set_facecolor('white')\n",
    "ax.bar(ind,vals,width=0.7)\n",
    "ax.set_title('SAMPLE TEST RESULTS\\n\\n'+params)\n",
    "ax.set_ylabel('Average')\n",
    "ax.set_ylim((0,100))\n",
    "ax.set_xticks(range(len(vals)))\n",
    "ax.set_xticklabels(x)\n",
    "for i, v in enumerate(vals):\n",
    "    plt.text(x=i-0.12, y =v+0.5 , s=f\"{v}\" , fontdict=dict(fontsize=12), fontweight='bold')\n",
    "#plt.savefig(\"Results with Confidence\", dpi=300)\n",
    "#plt.savefig(\"Results\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlBb6-jhAgE3"
   },
   "source": [
    "##### Complete Prediction List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZizJ3DcgAiFV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ufa3BbghAn_Q"
   },
   "source": [
    "##### Wrong Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pG1FHol7uzXe"
   },
   "outputs": [],
   "source": [
    "print('Wrong Predictions: ' + str(w_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohvMtBvEAj7e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(wrong_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_check.to_csv('prediction.csv', index=False)\n",
    "#wrong_pred.to_csv('wrong_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrkY_axDoA5k"
   },
   "source": [
    "##### Sample Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnlVgKvZoDGk"
   },
   "outputs": [],
   "source": [
    "def sample(ind):\n",
    "    for r in range(10):\n",
    "        print(\"  \"+ class_names[r] + \" = \" + str(predictions[ind][r]*100))\n",
    "    plt.figure(figsize=(10,4)).patch.set_facecolor('white')\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.grid(False)\n",
    "    plt.title('Test Image ' + str(ind))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(test_images[i], cmap=plt.cm.binary)\n",
    "    predicted_label = np.argmax(predictions[ind])   \n",
    "    if predicted_label == test_labels[ind]:\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'red' \n",
    "    confidence = 100*np.max(predictions[ind])\n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label], confidence, class_names[test_labels[ind]]), color=color)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.grid(False)\n",
    "    plt.title('Prediction Array')\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions[ind], color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[test_labels[ind]].set_color('green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphing the confidence of predictions\n",
    "#Choose an image and predict\n",
    "radnum = np.random.randint (0, 9999+1)\n",
    "print(\"Sample Test data prediction: \")\n",
    "sample(radnum)\n",
    "#plt.savefig(\"Sample Prediction\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_num = random.choice(index_list)\n",
    "print(\"Wrong Sample Test data prediction: \")\n",
    "sample(random_num)\n",
    "#plt.savefig(\"Wrong Sample Prediction\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_env",
   "language": "python",
   "name": "deep_learning_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
